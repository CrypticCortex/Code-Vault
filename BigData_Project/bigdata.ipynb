{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19837\n"
     ]
    }
   ],
   "source": [
    "DATASET_COLUMNS = ['target','ids','date','flag','user','text']\n",
    "\n",
    "dataset = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='ISO-8859-1', engine=\"python\", names=DATASET_COLUMNS)\n",
    "dataset = dataset[['target','text']]\n",
    "dataset.loc[dataset['target'] == 4, 'target'] = 1\n",
    "# take the first 10000 rows\n",
    "\n",
    "# take the last 10000 rows\n",
    "\n",
    "dataset = pd.concat([df_top, df_bottom])\n",
    "text = np.array(dataset.text)\n",
    "target = np.array(dataset.target)\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import string\n",
    "\n",
    "def preprocess(textdata):\n",
    "    processedText = np.empty(len(textdata), dtype=object)\n",
    "\n",
    "    # Defining regex patterns.\n",
    "    urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern = '@[^\\s]+'\n",
    "    alphaPattern = \"[^a-zA-Z0-9]\\s\"\n",
    "    sequencePattern = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "    wordLessThan2Pattern = r'\\b\\w{1,2}\\b'\n",
    "\n",
    "    for i, tweet in enumerate(textdata):\n",
    "        tweet = tweet.lower()\n",
    "\n",
    "        # Replace all URLs with nothing.\n",
    "        tweet = re.sub(urlPattern, \" \", tweet, flags=re.MULTILINE)\n",
    "        # Replace @USERNAME with nothing.\n",
    "        tweet = re.sub(userPattern, \" \", tweet)\n",
    "        # Replace all non-alphabets.\n",
    "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "        # Replace 3 or more consecutive letters by 2 letters.\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "        # Remove words less then 2 letters\n",
    "        tweet = re.sub(wordLessThan2Pattern, '', tweet)\n",
    "        # Remove punctuation.\n",
    "        tweet = tweet.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        \n",
    "\n",
    "\n",
    "        # Remove stopwords and lemmatize.\n",
    "        doc = nlp(tweet)\n",
    "        filteredWords = [token.lemma_ for token in doc if token.lower_ not in STOP_WORDS]\n",
    "\n",
    "        # Join the words of a tweet.\n",
    "        cleanTweet = \" \".join(filteredWords)\n",
    "\n",
    "        # Remove extra whitespace.\n",
    "        cleanTweet = re.sub(r'\\s+', ' ', cleanTweet).strip()\n",
    "\n",
    "        # Assign the tweet to the processedText array.\n",
    "        processedText[i] = cleanTweet\n",
    "\n",
    "    return processedText\n",
    "processedtext = preprocess(text)\n",
    "df = pd.DataFrame({ \"target\":target,\"text\":processedtext})\n",
    "processedtextSerie  = pd.Series(processedtext)\n",
    "emptyRows = df[df.text == \"\"].index\n",
    "df.drop(emptyRows,inplace=True)\n",
    "df[df.text == \"\"].index\n",
    "print(len(df))\n",
    "df.to_csv('cleanData.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cleanData.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bag_of_words = Counter()\n",
    "for tweet in df.text:\n",
    "    words = tweet.split()\n",
    "    bag_of_words.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word  count\n",
      "---------------\n",
      "aww 176\n",
      "bummer 22\n",
      "shoulda 4\n",
      "get 1145\n",
      "david 21\n",
      "carr 2\n",
      "day 1465\n",
      "upset 37\n",
      "update 128\n",
      "facebook 60\n",
      "texte 5\n",
      "cry 98\n",
      "result 25\n",
      "school 336\n",
      "today 1016\n"
     ]
    }
   ],
   "source": [
    "print('word  count')\n",
    "print('-'*15)\n",
    "for i, (key, value) in enumerate(bag_of_words.items()):\n",
    "    if i >= 15:\n",
    "        break\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aack</th>\n",
       "      <th>aaghh</th>\n",
       "      <th>aah</th>\n",
       "      <th>aahh</th>\n",
       "      <th>aahhdont</th>\n",
       "      <th>aaiwa</th>\n",
       "      <th>aaj</th>\n",
       "      <th>aaje</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aalsmeer</th>\n",
       "      <th>...</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoofinally</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zrovna</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zune</th>\n",
       "      <th>zuraidah</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zut</th>\n",
       "      <th>zyrtec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 17226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aack  aaghh  aah  aahh  aahhdont  aaiwa  aaj  aaje  aaliyah  aalsmeer  ...  \\\n",
       "0   0.0    0.0  0.0   0.0       0.0    0.0  0.0   0.0      0.0       0.0  ...   \n",
       "1   0.0    0.0  0.0   0.0       0.0    0.0  0.0   0.0      0.0       0.0  ...   \n",
       "2   0.0    0.0  0.0   0.0       0.0    0.0  0.0   0.0      0.0       0.0  ...   \n",
       "3   0.0    0.0  0.0   0.0       0.0    0.0  0.0   0.0      0.0       0.0  ...   \n",
       "4   0.0    0.0  0.0   0.0       0.0    0.0  0.0   0.0      0.0       0.0  ...   \n",
       "\n",
       "   zoo  zoofinally  zoom  zrovna  zumba  zune  zuraidah  zurich  zut  zyrtec  \n",
       "0  0.0         0.0   0.0     0.0    0.0   0.0       0.0     0.0  0.0     0.0  \n",
       "1  0.0         0.0   0.0     0.0    0.0   0.0       0.0     0.0  0.0     0.0  \n",
       "2  0.0         0.0   0.0     0.0    0.0   0.0       0.0     0.0  0.0     0.0  \n",
       "3  0.0         0.0   0.0     0.0    0.0   0.0       0.0     0.0  0.0     0.0  \n",
       "4  0.0         0.0   0.0     0.0    0.0   0.0       0.0     0.0  0.0     0.0  \n",
       "\n",
       "[5 rows x 17226 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'\\b[a-zA-Z]+\\b')\n",
    "\n",
    "# Fit and transform the vectorizer on the tweets\n",
    "tfidf_matrix = vectorizer.fit_transform(df.text)\n",
    "\n",
    "# Get the feature names from the vectorizer's vocabulary\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a pandas DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Print the TF-IDF DataFrame\n",
    "tfidf_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TF-IDF vectorizer to convert text data into numerical features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7487399193548387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_vectorized, y_train)\n",
    "y_pred = svm_model.predict(X_test_vectorized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [04:36<00:00,  9.54s/it]\n"
     ]
    }
   ],
   "source": [
    "X = df['text']\n",
    "y = df['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TF-IDF vectorizer to convert text data into numerical features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "X_train = X_train_vectorized.toarray()[0:1000]\n",
    "X_test = X_test_vectorized.toarray()[0:1000]\n",
    "y_train = y_train[0:1000]\n",
    "y_test = y_test[0:1000]\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "ExtraTreesClassifier               0.66               0.66     0.66      0.66   \n",
      "RandomForestClassifier             0.65               0.65     0.65      0.64   \n",
      "NearestCentroid                    0.64               0.64     0.64      0.64   \n",
      "DecisionTreeClassifier             0.64               0.64     0.64      0.63   \n",
      "XGBClassifier                      0.63               0.64     0.64      0.63   \n",
      "NuSVC                              0.63               0.63     0.63      0.63   \n",
      "BaggingClassifier                  0.63               0.63     0.63      0.62   \n",
      "LogisticRegression                 0.62               0.63     0.63      0.62   \n",
      "PassiveAggressiveClassifier        0.62               0.62     0.62      0.62   \n",
      "CalibratedClassifierCV             0.61               0.61     0.61      0.60   \n",
      "LinearSVC                          0.60               0.60     0.60      0.60   \n",
      "LGBMClassifier                     0.59               0.60     0.60      0.58   \n",
      "BernoulliNB                        0.58               0.60     0.60      0.54   \n",
      "SVC                                0.59               0.60     0.60      0.57   \n",
      "AdaBoostClassifier                 0.59               0.59     0.59      0.56   \n",
      "RidgeClassifierCV                  0.59               0.59     0.59      0.59   \n",
      "Perceptron                         0.59               0.59     0.59      0.59   \n",
      "RidgeClassifier                    0.58               0.58     0.58      0.58   \n",
      "ExtraTreeClassifier                0.57               0.58     0.58      0.56   \n",
      "LinearDiscriminantAnalysis         0.57               0.57     0.57      0.57   \n",
      "SGDClassifier                      0.58               0.57     0.57      0.54   \n",
      "GaussianNB                         0.56               0.56     0.56      0.56   \n",
      "QuadraticDiscriminantAnalysis      0.53               0.52     0.52      0.42   \n",
      "KNeighborsClassifier               0.49               0.51     0.51      0.34   \n",
      "LabelSpreading                     0.52               0.51     0.51      0.37   \n",
      "LabelPropagation                   0.52               0.51     0.51      0.37   \n",
      "DummyClassifier                    0.48               0.50     0.50      0.31   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "ExtraTreesClassifier                11.15  \n",
      "RandomForestClassifier               7.50  \n",
      "NearestCentroid                      1.00  \n",
      "DecisionTreeClassifier               2.00  \n",
      "XGBClassifier                       16.43  \n",
      "NuSVC                               75.43  \n",
      "BaggingClassifier                    6.99  \n",
      "LogisticRegression                   2.64  \n",
      "PassiveAggressiveClassifier          1.97  \n",
      "CalibratedClassifierCV              25.54  \n",
      "LinearSVC                            7.07  \n",
      "LGBMClassifier                       1.18  \n",
      "BernoulliNB                          1.08  \n",
      "SVC                                 74.61  \n",
      "AdaBoostClassifier                  13.47  \n",
      "RidgeClassifierCV                    1.92  \n",
      "Perceptron                           1.37  \n",
      "RidgeClassifier                      1.48  \n",
      "ExtraTreeClassifier                  0.93  \n",
      "LinearDiscriminantAnalysis           6.32  \n",
      "SGDClassifier                        1.50  \n",
      "GaussianNB                           1.13  \n",
      "QuadraticDiscriminantAnalysis        6.75  \n",
      "KNeighborsClassifier                 1.35  \n",
      "LabelSpreading                       1.45  \n",
      "LabelPropagation                     1.40  \n",
      "DummyClassifier                      0.79  \n"
     ]
    }
   ],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
